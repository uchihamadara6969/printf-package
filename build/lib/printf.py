def printf(i=None):
    if i == None:
        print("1. Build CNN model for sample dataset\n2. Implement transfer learning with pretrained CNN model\n3. Implement transfer learning for imge classification with CIFAR-10 dataset\n4. Apply transfer learning for dog breed identification dataset\n5. Build review sentiment classifier using transfer learning\n6. Apply transfer learning for IMDB dataset with word embeddings\n7. Create document summaries using transfer learning\n8. Perform Audio event classification with transfer learning\n\nExample: printf(1)")
        return
    data = {
    "1": "# -*- coding: utf-8 -*-\n\"\"\"Exp_01.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1PM5OZLTLInypV5y6pClXSj28mfGflGa6\n\"\"\"\n\n!pip install opendatasets -qq\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom keras.utils import image_dataset_from_directory\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport opendatasets as od\n\nod.download('https://www.kaggle.com/datasets/hemendrasr/pizza-vs-ice-cream')\n\ntrain_dir = r'/content/pizza-vs-ice-cream/dataset/train'\ntest_dir = r'/content/pizza-vs-ice-cream/dataset/test'\nval_dir = r'/content/pizza-vs-ice-cream/dataset/valid'\n\ntrain_generator = image_dataset_from_directory(train_dir, image_size=(128, 128), batch_size=32)\ntest_generator = image_dataset_from_directory(test_dir, image_size=(128, 128), batch_size=32)\nval_generator = image_dataset_from_directory(val_dir, image_size=(128, 128), batch_size=32)\n\nplt.figure(figsize=(10, 10))\nfor images, labels in val_generator:\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")\n\nmodel = Sequential([\n    Conv2D(64, (3, 3), input_shape=(128, 128, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.25),\n\n    Conv2D(128, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.25),\n\n    Conv2D(256, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.25),\n\n    Conv2D(512, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.25),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics='accuracy'\n)\n\nmodel.summary()\n\ncallback = EarlyStopping(\n                            monitor='val_accuracy',\n                            patience=5,\n                            restore_best_weights=True\n                        )\n\nlogs = model.fit(\n                  train_generator,\n                  epochs=25,\n                  validation_data=val_generator,\n                  callbacks=[callback],\n                )\n\nplt.title('Training Log')\nplt.plot(logs.history['loss'], label='Training Loss')\nplt.plot(logs.history['accuracy'], label='Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Score')\nplt.legend()\nplt.show()\n\nplt.title('Validation Log')\nplt.plot(logs.history['val_loss'], label='Validation Loss')\nplt.plot(logs.history['val_accuracy'], label='Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Score')\nplt.legend()\nplt.show()\n\nres = model.evaluate(test_generator)\n\naccuracy = res[1]\nprint(accuracy)\n\n",
    "2": "# -*- coding: utf-8 -*-\n\"\"\"TL_exp_02\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1OAbZpe2UysZbpzJOTCuR_ivbsQs7WFCd\n\"\"\"\n\n!pip install -qq opendatasets tensorflow-hub\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.preprocessing.image import ImageDataGenerator\nimport opendatasets as od\nfrom tensorflow.keras import layers, models\nimport tensorflow_hub as hub\n\nod.download(\"https://www.kaggle.com/datasets/hemendrasr/pizza-vs-ice-cream\")\n\ntrain_dir = '/content/pizza-vs-ice-cream/dataset/train'\ntest_dir = '/content/pizza-vs-ice-cream/dataset/test'\n\nfrom keras.utils import image_dataset_from_directory\ntrain_dataset = image_dataset_from_directory(train_dir, image_size=(600, 600), batch_size=32)\ntest_dataset = image_dataset_from_directory(test_dir, image_size=(600, 600), batch_size=32)\n\nplt.figure(figsize=(10, 10))\nfor images, labels in test_dataset:\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")\n\ndef create_resnet152_model(input_shape=(600, 600, 3)):\n    base_model = tf.keras.applications.ResNet152(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = False\n\n    model = models.Sequential([\n        base_model,\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\nresnet = create_resnet152_model()\n\nhistory_resnet = resnet.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=5\n)\n\ndef create_efficientnetb7_model(input_shape=(600, 600, 3)):\n    base_model = tf.keras.applications.EfficientNetB7(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = False\n\n    model = models.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\nefficientnet = create_efficientnetb7_model()\n\nhistory_efficientnet = efficientnet.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=5\n)\n\ndef plot(history, model_name):\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='train_accuracy')\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n    plt.title(f'{model_name} Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='train_loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.title(f'{model_name} Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.show()\n\nplot(history_resnet, 'ResNet152')\n\nplot(history_efficientnet, 'EfficientNet-B7')\n\nresnet152_eval = resnet.evaluate(test_dataset)\nefficientnetb7_eval = efficientnet.evaluate(test_dataset)\n\nmodels = ['ResNet152', 'EfficientNet-B7']\naccuracy = [resnet152_eval[1], efficientnetb7_eval[1]]\nloss = [resnet152_eval[0], efficientnetb7_eval[0]]\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.bar(models, accuracy, color=['blue', 'green'])\nplt.title('Model Accuracy Comparison')\nplt.xlabel('Model')\nplt.ylabel('Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.bar(models, loss, color=['blue', 'green'])\nplt.title('Model Loss Comparison')\nplt.xlabel('Model')\nplt.ylabel('Loss')\n\nplt.show()",
    "3": "# -*- coding: utf-8 -*-\n\"\"\"Ex_no_3_a_b.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1FISxKDMSb_AMaqdZbt08T2xmTL331HfB\n\"\"\"\n\n# @title See data\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']\n\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i])\n    plt.xlabel(class_names[train_labels[i][0]])\nplt.show()\n\n\"\"\"# 3a\"\"\"\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, datasets\nimport matplotlib.pyplot as plt\n\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_images, train_labels, epochs=10,\n                    validation_data=(test_images, test_labels))\n\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint(f'Test accuracy: {test_acc}')\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\n\n\"\"\"# 3b\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.keras.applications import EfficientNetB5\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\n\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n\nbase_model = EfficientNetB5(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\nx = base_model.output\nx = Flatten()(x)\nx = Dense(128, activation='relu')(x)\noutput = Dense(10, activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=output)\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_images, train_labels, epochs=10,\n                    validation_data=(test_images, test_labels))\n\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint(f'Test accuracy: {test_acc}')\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()",
    "4": "# -*- coding: utf-8 -*-\n\"\"\"TL_ex_no_4.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1TjBYScu2r1WDNv2bSPvAMhykOha1nB3w\n\n# Data Collection\n\"\"\"\n\n!pip install tensorflow keras efficientnet opendatasets -qq\n\nimport opendatasets as od\n\nod.download(\"https://www.kaggle.com/datasets/miljan/stanford-dogs-dataset-traintest\")\n\n\"\"\"# Transfer Learning\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nimport efficientnet.tfkeras as efn\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    '/content/stanford-dogs-dataset-traintest/cropped/cropped/train',\n    target_size=(224, 224),\n    batch_size=8,\n    class_mode='categorical'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    '/content/stanford-dogs-dataset-traintest/cropped/cropped/test',\n    target_size=(224, 224),\n    batch_size=8,\n    class_mode='categorical'\n)\n\nbase_model = efn.EfficientNetB5(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\nx = base_model.output\nx = Flatten()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(120, activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=predictions)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=15\n)\n\nmodel.evaluate(validation_generator)\n\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n\nplot_graphs(history, 'accuracy')\n\nplot_graphs(history, 'loss')",
    "5": "# -*- coding: utf-8 -*-\n\"\"\"TL_EX_NO_5.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1i4xl-U70aOjqgb9PTMdUpfzxSYXW343k\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, BatchNormalization, Dropout, Dense\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\ndf = pd.read_csv(\"Content/Data.csv\")\n\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    text = str(text)\n    text = re.sub(r'\\d+', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.lower()\n    text = [word for word in text.split() if word not in stop_words]\n    return ' '.join(text)\n\ntemp = list(df['text'])\ndf['text'] = [preprocess_text(text) for text in temp]\n\ndf = df[df['sentiment'] != -1]\ndf.loc[df['sentiment'] == -1, 'sentiment'] = 0\n\ndf.head()\n\nX = df['text'].values\ny = df['sentiment'].values\n\nmax_length = 100\ntokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(X)\nX_padded = pad_sequences(sequences, maxlen=max_length, padding='post')\n\nX_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n\nmodel = Sequential([\n    Embedding(input_dim=5000, output_dim=128),\n    Bidirectional(LSTM(32, return_sequences=True)),\n    BatchNormalization(),\n    Bidirectional(LSTM(64)),\n    Dropout(0.3),\n    Dense(512, activation='relu'),\n    Dropout(0.3),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n\ndef plot_graphs(history, metric):\n  plt.figure(figsize=(10, 6))\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  if metric == 'accuracy':\n    plt.ylim(0, 1)\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n\nplot_graphs(history, 'accuracy')\n\nplot_graphs(history, 'loss')",
    "6": "# -*- coding: utf-8 -*-\n\"\"\"TL_EX_NO_6\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1GJ4jJ3bc7WF_TIVrs6Uuv2kM0XXoTalv\n\"\"\"\n\n!git clone https://github.com/rohanrao619/Twitter_Sentiment_Analysis.git\n\nimport numpy as np\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\nimport matplotlib.pyplot as plt\n\nmax_words = 10000  # Number of words to consider as features\nmax_length = 100  # Cut off reviews after 100 words\n\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\nX_train_padded = pad_sequences(X_train, maxlen=max_length)\nX_test_padded = pad_sequences(X_test, maxlen=max_length)\n\ndef load_glove_embeddings(file_path, embedding_dim, word_index, max_words):\n    embeddings_index = {}\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n\n    embedding_matrix = np.zeros((max_words, embedding_dim))\n    for word, i in word_index.items():\n        if i < max_words:\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\nembedding_dim = 100\nglove_file = '/content/Twitter_Sentiment_Analysis/glove.6B.100d.txt'\nword_index = imdb.get_word_index()\n\nembedding_matrix = load_glove_embeddings(glove_file, embedding_dim, word_index, max_words)\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=max_words, output_dim=embedding_dim,\n                    weights=[embedding_matrix], input_length=max_length, trainable=False))\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train_padded, y_train, batch_size=64, epochs=15, validation_split=0.2)\n\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()",
    "7": "# -*- coding: utf-8 -*-\n\"\"\"TL_EXP_07.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1i76ywMjEHl3O08UaTR_vgPVfwkeyApSR\n\"\"\"\n\nimport tensorflow.keras.backend as K\nfrom nltk.tokenize import sent_tokenize\nimport tensorflow as tf\nimport nltk\nnltk.download('punkt')\n\nimdb_model = tf.keras.models.load_model('/content/drive/MyDrive/Temp/imdb_model.h5')\n\nfor layer in imdb_model.layers:\n    layer.trainable = False\n\ncorpus = [\n    \"The movie had a very strong start.\",\n    \"However, the plot quickly fell apart.\",\n    \"The acting was top-notch, especially the lead actor.\",\n    \"But the storyline was predictable and uninspiring.\",\n    \"Overall, the movie had good moments but was disappointing.\"\n]\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(corpus)\n\nsequences = tokenizer.texts_to_sequences(corpus)\nmax_length = 200\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\nmax_length = 200\ninput_shape = (max_length,)\ninputs = tf.keras.Input(shape=input_shape)\n\nx = imdb_model(inputs)\n\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nx = tf.keras.layers.Dense(32, activation='relu')(x)\nx = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nsummarization_model = tf.keras.Model(inputs=inputs, outputs=x)\n\nsummarization_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nsummarization_model.summary()\n\npredictions = summarization_model.predict(padded_sequences)\n\nsummary = [corpus[i] for i, score in enumerate(predictions) if score > 0.436]\n\nprint(\"Summary:\")\nfor sentence in summary:\n    print(sentence)",
    "8": "# -*- coding: utf-8 -*-\n\"\"\"TL Exp number 8.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/10QiAKQkTT9cTvX_vIHglGGybSCiSDgdV\n\"\"\"\n\n!pip install opendatasets\n\nimport pandas as pd\n\ndf= pd.read_csv('/content/urbansound8k/data.csv')\ndf.head(5)\n\nimport librosa\naudio_file_path='/content/urbansound8k/fold10/100648-1-0-0.wav'\nlibrosa_audio_data,librosa_sample_rate=librosa.load(audio_file_path)\n\nprint(librosa_audio_data)\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15, 5))\nplt.plot(librosa_audio_data)\n\n# @title Import Modules\nimport pandas as pd\nimport numpy as np\nimport os, librosa\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow import keras\nfrom keras.utils import to_categorical\nfrom keras import layers, Sequential\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.layers import Conv2D, MaxPooling2D,BatchNormalization\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nfrom tqdm import tqdm\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\nmfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=40)\n\nmfccs\n\n# @title DataPath\naudio_dataset_path = '/content/data'\nmetadata =  pd.read_csv('/content/urbansound8k/data.csv')\nmetadata.head()\n\ndef mfccExtract(file):\n    waveform, sampleRate = librosa.load(file_name)\n    features = librosa.feature.mfcc(y = waveform, sr = sampleRate, n_mfcc = 50)\n    return np.mean(features, axis = 1)\n\nextractAll = []\n\nfor index_num, row in tqdm(metadata.iterrows()):\n    # Constructing file path\n    file_name = os.path.join(audio_dataset_path, 'fold' + str(row['fold']), row['slice_file_name'])\n\n    # Extracting features and appending them\n    features = mfccExtract(file_name)\n    extractAll.append([features, row['class']])\n\nfeaturesDf = pd.DataFrame(extractAll, columns = ['Features', 'Class'])\nfeaturesDf.head()\n\nX=np.array(featuresDf['Features'].tolist())\nY=np.array(featuresDf['Class'].tolist())\n\nlabelencoder=LabelEncoder()\nY=to_categorical(labelencoder.fit_transform(Y))\n\nY\n\n### Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\nnum_labels=Y.shape[1]\n\nfrom tensorflow.keras.applications import VGG19\nfrom keras.layers import *\nbase_model = VGG19(weights='imagenet',\n                       include_top=False,\n                       input_shape=(32, 32, 3))\n\nfrom tensorflow.keras.models import  Model\n# Add classification layers on top of it\nx = base_model.output\nx = Flatten()(x)\nx = Dense(128, activation='relu',input_shape = (50,))(x)\noutput = Dense(10, activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=output)\n\nmodel.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n\nhistory = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs = 10)\n\ntest_accuracy=model.evaluate(X_test,Y_test,verbose=0)\nprint(test_accuracy[1])\n\nhistoryDf = pd.DataFrame(history.history)\n\nhistoryDf.loc[:, ['loss', 'val_loss']].plot()\n\nhistoryDf.loc[:, ['accuracy', 'val_accuracy']].plot()\n\n# Evaluating model\nscore = model.evaluate(X_test, Y_test)[1] * 100\nprint(f'Validation accuracy of model : {score:.2f}%')"
}
    t1 = type(i)
    if t1 == int or t1 == str:
        i = str(i)
    else:
        print("Traceback (Pass experiment number only):\n\tOnly pass integer values or strings\n\t\tExample: printf(1)")
        return
    try:
        print(data[i])
    except KeyError:
        print("Traceback (most recent call last):\n\tInvalid Key!!\n\tPlease enter a values between 1 to 8\n\t\tExample: printf(1)")
    except Exception as e:    
        print("An error occurred!!\n", e)    
