def printf(i=None):
    if i == None:
        print("1. Build CNN model for sample dataset\n2. Implement transfer learning with pretrained CNN model\n3. Implement transfer learning for imge classification with CIFAR-10 dataset\n4. Apply transfer learning for dog breed identification dataset\n5. Build review sentiment classifier using transfer learning\n6. Apply transfer learning for IMDB dataset with word embeddings\n7. Create document summaries using transfer learning\n8. Perform Audio event classification with transfer learning\n\nExample: printf(1)\n9. Apply transfer learning for IMDB dataset with GLoVe")
        return
    data = {
    "1": "# -*- coding: utf-8 -*-\n\"\"\"Exp_01.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1PM5OZLTLInypV5y6pClXSj28mfGflGa6\n\"\"\"\n\n!pip install opendatasets -qq\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom keras.utils import image_dataset_from_directory\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport opendatasets as od\n\nod.download('https://www.kaggle.com/datasets/hemendrasr/pizza-vs-ice-cream')\n\ntrain_dir = r'/content/pizza-vs-ice-cream/dataset/train'\ntest_dir = r'/content/pizza-vs-ice-cream/dataset/test'\nval_dir = r'/content/pizza-vs-ice-cream/dataset/valid'\n\ntrain_generator = image_dataset_from_directory(train_dir, image_size=(128, 128), batch_size=32)\ntest_generator = image_dataset_from_directory(test_dir, image_size=(128, 128), batch_size=32)\nval_generator = image_dataset_from_directory(val_dir, image_size=(128, 128), batch_size=32)\n\nplt.figure(figsize=(10, 10))\nfor images, labels in val_generator:\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")\n\nmodel = Sequential([\n    Conv2D(64, (3, 3), input_shape=(128, 128, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.25),\n\n    Conv2D(128, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.25),\n\n    Conv2D(256, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.25),\n\n    Conv2D(512, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.25),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\ncallback = EarlyStopping(\n                            monitor='val_accuracy',\n                            patience=5,\n                            restore_best_weights=True\n                        )\n\nlogs = model.fit(\n                  train_generator,\n                  epochs=2,\n                  validation_data=val_generator,\n                  callbacks=[callback],\n                )\n\nplt.title('Training Log')\nplt.plot(logs.history['loss'], label='Training Loss')\nplt.plot(logs.history['accuracy'], label='Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Score')\nplt.legend()\nplt.show()\n\nplt.title('Validation Log')\nplt.plot(logs.history['val_loss'], label='Validation Loss')\nplt.plot(logs.history['val_accuracy'], label='Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Score')\nplt.legend()\nplt.show()\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\nmodel.evaluate(test_generator)\n\ny_pred = model.predict(test_generator)\ny_pred_classes = np.where(y_pred > 0.5, 1, 0)\n# y_pred_classes = np.argmax(y_pred, axis=1)  # For multi-class classification\ny_true = np.concatenate([y for x, y in test_generator], axis=0)\n\nimport numpy as np\ny_pred_classes = np.where(y_pred > 0.5, 1, 0)\ny_true = np.concatenate([y for x, y in test_generator], axis=0)\n\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nprint('Classification Report')\nprint(classification_report(y_true, y_pred_classes))",
    "2": "# -*- coding: utf-8 -*-\n\"\"\"TL_exp_02\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1OAbZpe2UysZbpzJOTCuR_ivbsQs7WFCd\n\"\"\"\n\n!pip install -qq opendatasets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import *\nfrom keras.layers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport opendatasets as od\nfrom tensorflow.keras import layers, models\nimport tensorflow_hub as hub\n\nod.download(\"https://www.kaggle.com/datasets/hemendrasr/pizza-vs-ice-cream\")\n\ntrain_dir = '/content/pizza-vs-ice-cream/dataset/train'\ntest_dir = '/content/pizza-vs-ice-cream/dataset/test'\n\nfrom keras.utils import image_dataset_from_directory\ntrain_dataset = image_dataset_from_directory(train_dir, image_size=(600, 600), batch_size=32)\ntest_dataset = image_dataset_from_directory(test_dir, image_size=(600, 600), batch_size=32)\n\nplt.figure(figsize=(10, 10))\nfor images, labels in test_dataset:\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")\n\ninput_shape = (600, 600, 3)\n\nbase_model = tf.keras.applications.VGG16(\n    input_shape=input_shape,\n    include_top=False,\n    weights='imagenet'\n)\n\nbase_model = tf.keras.applications.InceptionV3(\n    input_shape=input_shape,\n    include_top=False,\n    weights='imagenet'\n)\n\nbase_model = tf.keras.applications.ResNet50(\n    input_shape=input_shape,\n    include_top=False,\n    weights='imagenet'\n)\n\nbase_model.trainable = False\n\nresnet = models.Sequential([\n    base_model,\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\nresnet.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory = resnet.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=1\n)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\nresnet.evaluate(test_dataset)\n\ny_pred = resnet.predict(test_dataset)\ny_pred_classes = np.where(y_pred > 0.5, 1, 0)\n# y_pred_classes = np.argmax(y_pred, axis=1)  # For multi-class classification\ny_true = np.concatenate([y for x, y in test_dataset], axis=0)\n\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nprint('Classification Report')\nprint(classification_report(y_true, y_pred_classes))",
    "3": "# -*- coding: utf-8 -*-\n\"\"\"Ex_no_3_a_b.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1FISxKDMSb_AMaqdZbt08T2xmTL331HfB\n\"\"\"\n\n# @title See data\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']\n\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i])\n    plt.xlabel(class_names[train_labels[i][0]])\nplt.show()\n\n\"\"\"# 3a\"\"\"\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, datasets\nimport matplotlib.pyplot as plt\n\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_images, train_labels, epochs=1,\n                    validation_data=(test_images, test_labels))\n\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint(f'Test accuracy: {test_acc}')\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport numpy as np\n\ny_pred = model.predict(test_images)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.concatenate(test_labels, axis=0)\n\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n# Classification Report\nprint('Classification Report')\nprint(classification_report(y_true, y_pred_classes))\n\n\"\"\"# 3b\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\n\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n\nx = base_model.output\nx = Flatten()(x)\nx = Dense(128, activation='relu')(x)\noutput = Dense(10, activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=output)\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_images, train_labels, epochs=1,\n                    validation_data=(test_images, test_labels))\n\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint(f'Test accuracy: {test_acc}')\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\n\ny_pred = model.predict(test_images)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.concatenate(test_labels, axis=0)\n\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n# Classification Report\nprint('Classification Report')\nprint(classification_report(y_true, y_pred_classes))",
    "4": "# -*- coding: utf-8 -*-\n\"\"\"exp 4 dogbreed.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1wbfgpzgaRgQsWHt-sB24lFEo9T3VKva6\n\"\"\"\n\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"hartman/dog-breed-identification\")\n\nprint(\"Path to dataset files:\", path)\n\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndata=pd.read_csv(r'C:\\Users\\srujo\\.cache\\kagglehub\\datasets\\hartman\\dog-breed-identification\\versions\\1\\labels.csv')\ndata.head()\n\ndata['id']=data['id'].apply(lambda x:x+'.jpg')\ndata.head()\n\nsize=(224,224)\n\ndatagen=ImageDataGenerator(rescale=1./255,\n                           rotation_range=20,\n                           shear_range=0.2,\n                           zoom_range=0.2,\n                           validation_split=0.3,\n                           fill_mode='nearest',\n                           horizontal_flip=True)\n\ntrain=datagen.flow_from_dataframe(data,directory=r'C:\\Users\\srujo\\.cache\\kagglehub\\datasets\\hartman\\dog-breed-identification\\versions\\1\\train',\n                                  x_col='id',\n                                  y_col='breed',\n                                  class_mode='categorical',\n                                  target_size=size,\n                                  batch_size=32,\n                                  subset='training')\n\ntest=datagen.flow_from_dataframe(data,directory=r'C:\\Users\\srujo\\.cache\\kagglehub\\datasets\\hartman\\dog-breed-identification\\versions\\1\\train',\n                                 x_col='id',\n                                 y_col='breed',\n                                 class_mode='categorical',\n                                 target_size=size,\n                                 batch_size=32,\n                                 subset='validation')\n\nimport tensorflow\n\nvgg=VGG16(weights='imagenet',include_top=False,input_shape=(64,64,3))\nvgg.trainable=False\nmodel=vgg.output\nmodel=GlobalAveragePooling2D()(model)\nmodel=Dropout(0.2)(model)\nmodel=Dense(512,activation='relu')(model)\npred=Dense(120,activation='softmax')(model)\nmm=tensorflow.keras.models.Model(vgg.input,pred)\n\nmm.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\ner=EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)\nhis=mm.fit(train,epochs=5,validation_data=test,callbacks=[er])\n\nimport matplotlib.pyplot as plt\nimg, label = next(train)\n\nfig = plt.figure(figsize=(15, 10))\n\nfor i in range(12):\n    fig.add_subplot(3, 4, i+1)\n    plt.imshow(img[i])\n    plt.axis('off')",
    "5": "# -*- coding: utf-8 -*-\n\"\"\"Ex_No_5_Sentiment_Analysis.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1AJMOmvLah8XA7HEtj_M9q9bSK98CG2Qt\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport collections\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom gensim.models import Word2Vec\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom tabulate import tabulate\n\nimport pandas as pd\n\ndf_train = pd.read_csv('/content/train.csv')\ndf_test = pd.read_csv('/content/test.csv')\n\ndf_train.head()\n\ndf_train.info()\n\nprint(df_train['1'].value_counts())\nprint(df_test['1'].value_counts())\n\n# Preprocess text\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    text = re.sub(r'\\d+', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.lower()\n    text = [word for word in text.split() if word not in stop_words]\n    return ' '.join(text)\n\ndf_train['processed_text'] = df_train['0'].apply(preprocess_text)\ndf_test['processed_text'] = df_test['0'].apply(preprocess_text)\n\n# Tokenize and pad sequences\nvocab_size = 10000\nmax_len = 200  # maximum length of a sequence\nembedding_dim = 16\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\ntokenizer.fit_on_texts(df_train['processed_text'])\n\ntrain_seq = tokenizer.texts_to_sequences(df_train['processed_text'])\ntest_seq = tokenizer.texts_to_sequences(df_test['processed_text'])\n\ntrain_pad = pad_sequences(train_seq, maxlen=max_len, padding='post')\ntest_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\n\nfrom tensorflow.keras.utils import to_categorical\n\ntrain_label = to_categorical(df_train['1'])\ntest_label = to_categorical(df_test['1'])\n\ny_train = df_train['1'].values\ny_train\n\n# Build the model\nmodel = tf.keras.models.Sequential([\n    Embedding(vocab_size, embedding_dim, input_length=max_len),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.summary()\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n\nhistory = model.fit(train_pad, df_train['1'].values, epochs=10, batch_size=64, validation_split=0.1)\n\n# Plot training history\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show()\n\n# Evaluate the model\ntest_seq_padded = pad_sequences(test_seq, maxlen=max_len, padding='post')\ntest_seq_np = np.array(test_seq_padded)\n\npredictions = model.predict(test_seq_np)\npredicted_labels = np.round(predictions)\ntrue_labels = np.array(df_test['1'])\n\ncm = confusion_matrix(true_labels, predicted_labels)\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"size\": 16})\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n\naccuracy = accuracy_score(true_labels, predicted_labels)\nprecision = precision_score(true_labels, predicted_labels)\nrecall = recall_score(true_labels, predicted_labels)\nf1 = f1_score(true_labels, predicted_labels)\n\ntable = [\n    [\"Accuracy\", accuracy],\n    [\"Precision\", precision],\n    [\"Recall\", recall],\n    [\"F1-score\", f1]\n]\n\nprint(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))",
    "9": "# -*- coding: utf-8 -*-\n\"\"\"TL_EX_NO_6\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1GJ4jJ3bc7WF_TIVrs6Uuv2kM0XXoTalv\n\"\"\"\n\n!git clone https://github.com/rohanrao619/Twitter_Sentiment_Analysis.git\n\nimport numpy as np\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\nimport matplotlib.pyplot as plt\n\nmax_words = 10000  # Number of words to consider as features\nmax_length = 100  # Cut off reviews after 100 words\n\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\nX_train_padded = pad_sequences(X_train, maxlen=max_length)\nX_test_padded = pad_sequences(X_test, maxlen=max_length)\n\ndef load_glove_embeddings(file_path, embedding_dim, word_index, max_words):\n    embeddings_index = {}\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n\n    embedding_matrix = np.zeros((max_words, embedding_dim))\n    for word, i in word_index.items():\n        if i < max_words:\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\nembedding_dim = 100\nglove_file = '/content/Twitter_Sentiment_Analysis/glove.6B.100d.txt'\nword_index = imdb.get_word_index()\n\nembedding_matrix = load_glove_embeddings(glove_file, embedding_dim, word_index, max_words)\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=max_words, output_dim=embedding_dim,\n                    weights=[embedding_matrix], input_length=max_length, trainable=False))\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train_padded, y_train, batch_size=64, epochs=15, validation_split=0.2)\n\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()",
    "6": "# -*- coding: utf-8 -*-\n\"\"\"Ex_No_6_Transfer_learning_with_word_embedding.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1ZkZ-h9dlHa_9KBvb2Btq_qONUxu_Fukr\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport collections\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom gensim.models import Word2Vec\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom tabulate import tabulate\n\nimport pandas as pd\n\ndf_train = pd.read_csv('/content/train.csv')\ndf_test = pd.read_csv('/content/test.csv')\n\ndf_train.head()\n\ndf_train.info()\n\nprint(df_train['1'].value_counts())\nprint(df_test['1'].value_counts())\n\n# Preprocess text\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    text = re.sub(r'\\d+', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.lower()\n    text = [word for word in text.split() if word not in stop_words]\n    return ' '.join(text)\n\ndf_train['processed_text'] = df_train['0'].apply(preprocess_text)\ndf_test['processed_text'] = df_test['0'].apply(preprocess_text)\n\n# Train Word2Vec model\nword2vec_model = Word2Vec(sentences=df_train['processed_text'], vector_size=100, window=5, min_count=1, workers=4)\nword2vec_model.save(\"word2vec.model\")\n\n# Create embedding matrix\nvocab_size = 10000\nembedding_dim = 100\nword_index = {word: i+1 for i, word in enumerate(word2vec_model.wv.index_to_key)}\n\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in word_index.items():\n    if i < vocab_size:\n        try:\n            embedding_vector = word2vec_model.wv[word]\n            embedding_matrix[i] = embedding_vector\n        except KeyError:\n            embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embedding_dim)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\ntokenizer.fit_on_texts(df_train['processed_text'])\n\ntrain_seq = tokenizer.texts_to_sequences(df_train['processed_text'])\ntest_seq = tokenizer.texts_to_sequences(df_test['processed_text'])\n\naverage_len = np.mean([len(seq) for seq in train_seq])\nmax_len = int(average_len + 100)\n\ntrain_pad = pad_sequences(train_seq, maxlen=max_len, padding='post')\ntest_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\n\nfrom tensorflow.keras.utils import to_categorical\n\ntrain_label = to_categorical(df_train['1'])\ntest_label = to_categorical(df_test['1'])\n\ny_train = df_train['1'].values\ny_train\n\n# Build the model\nmodel = tf.keras.models.Sequential([\n    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.summary()\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n\nhistory = model.fit(train_pad, df_train['1'].values, epochs=10, batch_size=64, validation_split=0.1)\n\n# Plot training history\nplt.style.use('dark_background')\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show()\n\nplt.style.use('dark_background')\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show()\n\n\n\n# Evaluate the model\ntest_seq_padded = pad_sequences(test_seq, maxlen=max_len, padding='post')\ntest_seq_np = np.array(test_seq_padded)\n\npredictions = model.predict(test_seq_np)\npredicted_labels = np.round(predictions)\ntrue_labels = np.array(df_test['1'])\n\ncm = confusion_matrix(true_labels, predicted_labels)\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"size\": 16})\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n\naccuracy = accuracy_score(true_labels, predicted_labels)\nprecision = precision_score(true_labels, predicted_labels)\nrecall = recall_score(true_labels, predicted_labels)\nf1 = f1_score(true_labels, predicted_labels)\n\ntable = [\n    [\"Accuracy\", accuracy],\n    [\"Precision\", precision],\n    [\"Recall\", recall],\n    [\"F1-score\", f1]\n]\n\nprint(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))",
    "7": "# -*- coding: utf-8 -*-\n\"\"\"TL_EXP_07 (1).ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/10WXbEqgITSyl7Wfpwe4stKKB1msom0m8\n\n# Train Model for TL\n\"\"\"\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nimdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\ntrain_data, test_data = imdb['train'], imdb['test']\n\ntrain_sentences = []\ntrain_labels = []\n\nfor s, l in train_data:\n    train_sentences.append(str(s.numpy()))\n    train_labels.append(l.numpy())\n\ntest_sentences = []\ntest_labels = []\n\nfor s, l in test_data:\n    test_sentences.append(str(s.numpy()))\n    test_labels.append(l.numpy())\n\nvocab_size = 10000\nmax_length = 200\nembedding_dim = 16\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nnum_epochs = 5\nmodel.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(test_padded, test_labels))\n\n# Save the trained model\nmodel.save(\"/content/drive/MyDrive/Temp/imdb_model.h5\")\n\n\"\"\"# Use Trained Model To implement TL\"\"\"\n\nimport tensorflow.keras.backend as K\nfrom nltk.tokenize import sent_tokenize\nimport tensorflow as tf\nimport nltk\nnltk.download('punkt')\n\nimdb_model = tf.keras.models.load_model('/content/drive/MyDrive/Temp/imdb_model.h5')\n\nfor layer in imdb_model.layers:\n    layer.trainable = False\n\ncorpus = [\n    \"The movie had a very strong start.\",\n    \"However, the plot quickly fell apart.\",\n    \"The acting was top-notch, especially the lead actor.\",\n    \"But the storyline was predictable and uninspiring.\",\n    \"Overall, the movie had good moments but was disappointing.\"\n]\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(corpus)\n\nsequences = tokenizer.texts_to_sequences(corpus)\nmax_length = 200\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\nmax_length = 200\ninput_shape = (max_length,)\ninputs = tf.keras.Input(shape=input_shape)\n\nx = imdb_model(inputs)\n\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nx = tf.keras.layers.Dense(32, activation='relu')(x)\nx = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nsummarization_model = tf.keras.Model(inputs=inputs, outputs=x)\n\nsummarization_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nsummarization_model.summary()\n\npredictions = summarization_model.predict(padded_sequences)\n\nsummary = [corpus[i] for i, score in enumerate(predictions) if score > 0.436]\n\nprint(\"Summary:\")\nfor sentence in summary:\n    print(sentence)",
    "8": "# -*- coding: utf-8 -*-\n\"\"\"TL Exp number 8.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/10QiAKQkTT9cTvX_vIHglGGybSCiSDgdV\n\"\"\"\n\n!pip install opendatasets\n\nimport pandas as pd\n\ndf= pd.read_csv('/content/urbansound8k/data.csv')\ndf.head(5)\n\nimport librosa\naudio_file_path='/content/urbansound8k/fold10/100648-1-0-0.wav'\nlibrosa_audio_data,librosa_sample_rate=librosa.load(audio_file_path)\n\nprint(librosa_audio_data)\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15, 5))\nplt.plot(librosa_audio_data)\n\n# @title Import Modules\nimport pandas as pd\nimport numpy as np\nimport os, librosa\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow import keras\nfrom keras.utils import to_categorical\nfrom keras import layers, Sequential\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.layers import Conv2D, MaxPooling2D,BatchNormalization\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nfrom tqdm import tqdm\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\nmfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=40)\n\nmfccs\n\n# @title DataPath\naudio_dataset_path = '/content/data'\nmetadata =  pd.read_csv('/content/urbansound8k/data.csv')\nmetadata.head()\n\ndef mfccExtract(file):\n    waveform, sampleRate = librosa.load(file_name)\n    features = librosa.feature.mfcc(y = waveform, sr = sampleRate, n_mfcc = 50)\n    return np.mean(features, axis = 1)\n\nextractAll = []\n\nfor index_num, row in tqdm(metadata.iterrows()):\n    # Constructing file path\n    file_name = os.path.join(audio_dataset_path, 'fold' + str(row['fold']), row['slice_file_name'])\n\n    # Extracting features and appending them\n    features = mfccExtract(file_name)\n    extractAll.append([features, row['class']])\n\nfeaturesDf = pd.DataFrame(extractAll, columns = ['Features', 'Class'])\nfeaturesDf.head()\n\nX=np.array(featuresDf['Features'].tolist())\nY=np.array(featuresDf['Class'].tolist())\n\nlabelencoder=LabelEncoder()\nY=to_categorical(labelencoder.fit_transform(Y))\n\nY\n\n### Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\nnum_labels=Y.shape[1]\n\nfrom tensorflow.keras.applications import VGG19\nfrom keras.layers import *\nbase_model = VGG19(weights='imagenet',\n                       include_top=False,\n                       input_shape=(32, 32, 3))\n\nfrom tensorflow.keras.models import  Model\n# Add classification layers on top of it\nx = base_model.output\nx = Flatten()(x)\nx = Dense(128, activation='relu',input_shape = (50,))(x)\noutput = Dense(10, activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=output)\n\nmodel.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n\nhistory = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs = 10)\n\ntest_accuracy=model.evaluate(X_test,Y_test,verbose=0)\nprint(test_accuracy[1])\n\nhistoryDf = pd.DataFrame(history.history)\n\nhistoryDf.loc[:, ['loss', 'val_loss']].plot()\n\nhistoryDf.loc[:, ['accuracy', 'val_accuracy']].plot()\n\n# Evaluating model\nscore = model.evaluate(X_test, Y_test)[1] * 100\nprint(f'Validation accuracy of model : {score:.2f}%')"
}
    t1 = type(i)
    if t1 == int or t1 == str:
        i = str(i)
    else:
        print("Traceback (Pass experiment number only):\n\tOnly pass integer values or strings\n\t\tExample: printf(1)")
        return
    try:
        print(data[i])
    except KeyError:
        print("Traceback (most recent call last):\n\tInvalid Key!!\n\tPlease enter a values between 1 to 9\n\t\tExample: printf(1)")
        print("1. Build CNN model for sample dataset\n2. Implement transfer learning with pretrained CNN model\n3. Implement transfer learning for imge classification with CIFAR-10 dataset\n4. Apply transfer learning for dog breed identification dataset\n5. Build review sentiment classifier using transfer learning\n6. Apply transfer learning for IMDB dataset with word embeddings\n7. Create document summaries using transfer learning\n8. Perform Audio event classification with transfer learning\n\nExample: printf(1)\n9. Apply transfer learning for IMDB dataset with GLoVe")

    except Exception as e:    
        print("An error occurred!!\n", e)    
